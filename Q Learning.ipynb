{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the environment\n",
    "\n",
    "from enum import Enum\n",
    "from lib2to3.pgen2.token import N_TOKENS\n",
    "\n",
    "class Action(Enum):\n",
    "    right = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    up = 3\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.height = 3\n",
    "        self.width = 4\n",
    "        self.rewards = np.zeros((self.height, self.width))\n",
    "        self.rewards[0, self.width-1] = 1\n",
    "        self.rewards[1, self.width-1] = -1\n",
    "        self.rewards[1, 1] = None\n",
    "        self.state = (self.height-1, 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (self.height-1, 0)\n",
    "    \n",
    "    def checkOutBound(self, state):\n",
    "        if state[0] < 0 or state[0] > self.height-1:\n",
    "            return True\n",
    "        if state[1] < 0 or state[1] > self.width-1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def checkWall(self, state):\n",
    "        if state == (1, 1):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def checkDone(self, state):\n",
    "        if self.rewards[state] == 1:\n",
    "            return True\n",
    "        if self.rewards[state] == -1:\n",
    "            return True\n",
    "        return False \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == Action.right:\n",
    "            newState = (self.state[0], self.state[1]+1)\n",
    "        elif action == Action.down:\n",
    "            newState = (self.state[0]+1, self.state[1])\n",
    "        elif action == Action.left:\n",
    "            newState = (self.state[0], self.state[1]-1)\n",
    "        elif action == Action.up:\n",
    "            newState = (self.state[0]-1, self.state[1])\n",
    "        \n",
    "        if self.checkOutBound(newState):\n",
    "            newState = self.state\n",
    "\n",
    "        if self.checkWall(newState):\n",
    "            newState = self.state\n",
    "        \n",
    "        self.state = newState\n",
    "\n",
    "        done = self.checkDone(newState)\n",
    "        \n",
    "        reward = self.rewards[newState]\n",
    "        \n",
    "        return (newState, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all variables\n",
    "episodes = 1000\n",
    "gamma = 0.9\n",
    "maxStep = 100\n",
    "\n",
    "epsilon = 1.0\n",
    "maxEpsilon = 1.0\n",
    "minEpsilon = 0.1\n",
    "decay = 1 / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "qTable = np.random.rand(env.height, env.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(state):\n",
    "    array = []\n",
    "    actions = []\n",
    "    if state[1]+1 <= qTable.shape[1]-1:\n",
    "        array.append(qTable[state[0],state[1]+1] + env.rewards[state[0],state[1]+1])\n",
    "        actions.append(Action.right)\n",
    "    if state[0]+1 <= qTable.shape[0]-1:\n",
    "        array.append(qTable[state[0]+1, state[1]] + env.rewards[state[0]+1, state[1]])\n",
    "        actions.append(Action.down)\n",
    "    if state[1]-1 >= 0:\n",
    "        array.append(qTable[state[0],state[1]-1] + env.rewards[state[0],state[1]-1])\n",
    "        actions.append(Action.left)\n",
    "    if state[0]-1 >= 0:\n",
    "        array.append(qTable[state[0]-1, state[1]] + env.rewards[state[0]-1, state[1]])\n",
    "        actions.append(Action.up)\n",
    "    return actions[np.argmax(array)]\n",
    "\n",
    "def getRandomA(state):\n",
    "    actions = []\n",
    "    if state[1]+1 <= qTable.shape[1]-1:\n",
    "        actions.append(Action.right)\n",
    "    if state[0]+1 <= qTable.shape[0]-1:\n",
    "        actions.append(Action.down)\n",
    "    if state[1]-1 >= 0:\n",
    "        actions.append(Action.left)\n",
    "    if state[0]-1 >= 0:\n",
    "        actions.append(Action.up)\n",
    "    return actions[np.random.randint(0, len(actions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    env.reset()\n",
    "    for step in range(maxStep):\n",
    "        state = env.state\n",
    "        tradeOff = random.uniform(0, 1)\n",
    "        if tradeOff > epsilon:\n",
    "            action = getAction(state)\n",
    "        else:\n",
    "            action = getRandomA(state)\n",
    "        newState, reward, done = env.step(action)\n",
    "        if reward == None:\n",
    "            continue\n",
    "        qTable[state] = reward + gamma * qTable[newState]\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "        print(qTable)\n",
    "    epsilon = minEpsilon + (maxEpsilon - minEpsilon) * np.exp(-decay*episode)\n",
    "print(qTable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3eec6905afc42fe5cdadc178736cc93ef800e9ca9362a6d50a5159adfbee55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
